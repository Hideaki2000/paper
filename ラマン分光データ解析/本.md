@import "../local.less"
## Majorization Minimization
> http://yaroslavvb.com/papers/hunter-tutorial.pdf
## せいせい
## ベースライン補正

### Baseline estimation and denoising:problem formulation

$$\begin{equation}
  \bold{s}=\bold{x}+\bold{f},\space \bold{s}\in \R^N
\end{equation}$$

the vector $\bold{x}$,consisting of numerous peaks, is modeled as a sparse-devrivative signal.The vector $\bold{f}$,representating the baseline , is low-pass signal. We further model the observed chromatogra data as

$$\begin{equation}
  \bold{y}=\bold{s}+\bold{f}+\bold{w},\space \bold{y} \in \R^N
\end{equation}$$
where $\bold{w}$ is stationary white Gaussian process with variance $\sigma^2$. Our goal is to estimate the baseline,$\bold{f}$ and peaks, $\bold{x}$,simultaneously,from observation $\bold{y}$.
 We assume that if peaks are absent, then the baseline can be approximately recovered from a noise-corrupted observation from by low-pass filtering. we may obtain an estimate $\hat{\bold{f}}$ of baseline by filtering $\bold{y}$ in (7) in with low pass filter $\bold{L}$,

 $$\begin{equation}
   \hat{\bold{f}}=\bold{L}(\bold{y}-\hat{\bold{x}})
 \end{equation}
 $$
 In this case, we may obtain an etimate $\hat{\bold{s}}$ by adding $\hat{\bold{x}}$

from (1)
$$\begin{equation}
  \begin{aligned}
    \hat{\bold{s}}&=\hat{\bold{x}}+\hat{\bold{f}}\\
                   &=\bold{L}(\bold{y}-\hat{\bold{x}})+\hat{\bold{x}}\\
                   &=\bold{L}(\bold{y})-(\bold{I}-\bold{L})(\hat{\bold{x}})\\
                   &=\bold{L}(\bold{y})+\bold{H}(\hat{\bold{x}}) \space (\bold{H}=\bold{I}-\bold{L})
  \end{aligned}
\end{equation}
$$
where $\bold{H}$ is high pass filter.
　In order to obtain $\hat{\bold{x}}$ of chromatgraph peaks from the observed data $\bold{y}$, We will formulate an inverse problem with the quadratic data fidelity term $\|\bold{y}-\hat{\bold{s}}\|$. Note that

$$\begin{equation}
\begin{aligned}
\|\bold{y}-\hat{\bold{s}}\|_2^2&=\|\bold{y}-\bold{Ly}-\bold{H\hat{x}}\|_2^2\\
                              &=\|\bold{H}(\bold{y}-\bold{\hat{x}})\|_2^2  
\end{aligned}
\end{equation}$$

Hence,the data fidelity term in the proposed formulation depends on the high-pass filter $\bold{H}$.Also note that the data fidelity term does not depend on the baseline estimate $\hat{\bold{f}}$. The optimization problem,formulated below. preduces an estimate of the chromatogram peaks $\hat{\bold{x}}$. The baseline estimate is then obtained by (3)

### Compound sparse derivative modeling
 According to (1),the first i derivatives, $i=0,\dotsb,M$ of the esimated peaks,should be sparse. In sparse signal processing, sparse signal behavior is typically acheved through the use of suitable non-quadratic regulation terms. Therefore, to obtain an estimate $\hat{\bold{x}}$, the following optimization problem is proposed:
 $$\begin{equation}
 \hat{\bold{x}}=arg\space \min\left(F(\bold{x})=\frac{1}{2}\|\bold{H}(\bold{y}-\bold{x})\|_2^2+\sum^M_{i=0}\lambda_i\bold{R}_i(\bold{D_i\bold{x}})\right)
 \end{equation}$$
 The assumptiion that the observed data is corruptedn by additive white Gaussian noise(AWGN) is reflected in the use of a quadratic data fitting term as in classical. The quadratic data fidelity term is given by (5).
 $\bold{D}_i$ is the order-i difference operator.
 Functions $R_i:\R^{N-i}\rightarrow\R$, are of the form
 $$\begin{equation}
   R_i(\bold{v})=\sum_n\phi(\bold{v}_n)
 \end{equation}$$
where $N_i$ denotes the length of $\bold{D_i}x$. The constants $\lambda_i\geq 0$ are regulation parameters. Increasing $\lambda_i$ has the effect of making $\bold{D}_i x$ more sparse.
 this work introduces several modifications to adapt the approach therein to chromatograms. The novel features of the approach proposed here include: 
 
 - an <em class="que">M-term compound regularizer </em> to model chromatogram peaks
 - The modeling of the positivity of chromatgram peaks by non-symmetric penalties
 - An improved algorithm based on MM for compound regularization
  
### Symmetric penalty functions

For many applications, samples of the targe signal $\bold{x}$ and its derivative $\bold{D_ix}$ are positive or negative with equal probability, or this information is not available. In such cases, the penalty function should be symmetric about $x=0$. One such function is the absolute value function,
$$\begin{equation}
  \phi_A(x)=|x|
\end{equation}$$
which leads to $l_1$ norm regularization. While widely used, one drawback of (8) is that it is non-differentialbe at zero, which can lead to numerical issues, depending on the utilized optimization algorithm. To address this issue, we utilize a smoothed approximation of the $l_1$ penalty function,for example, the hypebolic function
$$\begin{equation}
\phi_B(x)=\sqrt{|x|^2+\epsilon}
\end{equation}$$
or
$$\begin{equation}
  \phi_C(x)=|x|-\epsilon\log(|x|+\epsilon)
\end{equation}$$

In this work. They have found that $\epsilon=10^{-5}$ workds well with both (9),(10)/

### Asymmetric penalty functions
 For some applications, the signal $\bold{x}$ may be known to be sparse in an asymmetric manner. Formexample, it is known that chrotatogram data have positive peaks adove a relatively flat baseline. In such acases, it is preferable to use an asymmetric penalty function that penalizes positive and negative values diffferently.
 We start with the function $\theta:\R\rightarrow\R$ defined by

 $$\begin{equation}
 \theta(x;r)=\left\{
          \begin{array}{ll}
              x & x \geq 0 \\
              -rx & x\leq 0
          \end{array}
            \right.
 \end{equation}$$
where $r>0$ is a parameter. The function $\theta(x;r)$ is convex in $x$ and penalizes the positive and negative amplitudes asymmetrically. If r=1, then $\theta(x;r)=\phi_A(x)=|x|$
The definition (24) has the same drawback as (21), that is, it is non-differentilable at x=0. To alleviate this, a differentiable version of (24) is also propssed. In contrast to the differentiable symmetric penalties (9),(10), Thet function they propse is of the form
$$\begin{equation}
  \theta_{\epsilon}(x;r)=\left\{\begin{array}{}
  x & x>\epsilon\\
  f(x) &  |x|\leq\epsilon \\
  -rx & x< -\epsilon  
  \end{array}\right.
\end{equation}$$
with this modification, 

- $\theta_{\epsilon}(x;r)$ will remain convex in $(-\infty,+\infty)$
- $\theta_{\epsilon}(x;r)$ will be continuously differentialable on $(-\infty, +\infty)$
  
## Algorithms

### Symmetric penalty function

 The first form of the propssed approach,BEADS, is defined through the ,BEADS is defined through the minimizatiion of the objective function $F$,where $\phi$ is a differentiable symmetric penalty function. In this section,we use te MM procedure to derive an iterative algorithm for this optimization. Hence, we seek a majorizer $G(\bold{x},\bold{v})$ of $F(\bold{x})$. First, we find a majorizer $g(x,v)\text{for}\phi(x):\R\rightarrow\R \text{such that}$
 $$\begin{equation}
   g(x,v)>\phi(x)
 \end{equation}$$

 $$\begin{equation}
   g(v,v)=\phi(v), \forall x,v \in \R
 \end{equation}$$

 Since $\phi(x)$ is symmetric, we set $g(x,v)$ to be an even second-order polynomial

 $$\begin{equation}
   g(x,v)=mx^2+b
 \end{equation}$$
  
  where m and c are to be determined so as to satisfy (13),(14)
  we have
  $$g(v,v)=\phi(v)\space \text{and} \space g^{'}(v,v)=\phi^{'}(v)$$
  
  so Solving for $m$ and $b$ we obtain
  $$\begin{equation}
    m=\frac{\phi^{'}(v)}{2v}\space\text{and}\space b=\phi(v)-\frac{v}{2}\phi^{'}(v)
  \end{equation}$$
  
  Substituting (16) in (15), we obtain
  $$\begin{equation}
    g(x,v)=\frac{\phi^{'}(v)}{2v}x^2+\phi(v)-\frac{v}{2}\phi^{'}(v)
  \end{equation}$$

> [ベースライン補正](https://doi.org/10.1016/j.chemolab.2014.09.014)