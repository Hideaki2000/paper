@import "../local.less"
## Majorization Minimization
凸最適化の一つであり，漸近性を保証されているMMアルゴリズムを採用している
> http://yaroslavvb.com/pas/hunter-tutorial.pdf
## せいせい
## ベースライン補正
## 日本語まとめ
 ノイズの存在しないスペクトルを仮定する．
 $$\begin{equation}
   \bold{s}=\bold{x}+\bold{f},\space \bold{s} \in \R^N
 \end{equation}$$

 ここで$\bold{x}$はいろいろなピークの値であり，必然てきにその$\bold{x}$は疎行列になる．また$\bold{f}$はベースラインをしめしており，ローパスシグナルである．つぎにノイズを含んだスペクトルのデータをしたのようにモデル化する・
 $$\begin{equation}
   \bold{y}=\bold{s}+\bold{w}
 \end{equation}$$

 $\bold{w}$は白色ガウス雑音，$\bold{y}$を観測した値とする．
 
 ここでThe d

### Baseline estimation and denoising:problem formulation

$$\begin{equation}
  \bold{s}=\bold{x}+\bold{f},\space \bold{s}\in \R^N
\end{equation}$$

the vector $\bold{x}$,consisting of numerous peaks, is modeled as a sparse-devrivative signal.The vector $\bold{f}$,representating the baseline , is low-pass signal. We further model the observed chromatogra data as

$$\begin{equation}
  \bold{y}=\bold{s}+\bold{f}+\bold{w},\space \bold{y} \in \R^N
\end{equation}$$
where $\bold{w}$ is stationary white Gaussian process with variance $\sigma^2$. Our goal is to estimate the baseline,$\bold{f}$ and peaks, $\bold{x}$,simultaneously,from observation $\bold{y}$.
 We assume that if peaks are absent, then the baseline can be approximately recovered from a noise-corrupted observation from by low-pass filtering. we may obtain an estimate $\hat{\bold{f}}$ of baseline by filtering $\bold{y}$ in (7) in with low pass filter $\bold{L}$,

 $$\begin{equation}
   \hat{\bold{f}}=\bold{L}(\bold{y}-\hat{\bold{x}})
 \end{equation}
 $$
 In this case, we may obtain an etimate $\hat{\bold{s}}$ by adding $\hat{\bold{x}}$

from (1)
$$\begin{equation}
  \begin{aligned}
    \hat{\bold{s}}&=\hat{\bold{x}}+\hat{\bold{f}}\\
                   &=\bold{L}(\bold{y}-\hat{\bold{x}})+\hat{\bold{x}}\\
                   &=\bold{L}(\bold{y})-(\bold{I}-\bold{L})(\hat{\bold{x}})\\
                   &=\bold{L}(\bold{y})+\bold{H}(\hat{\bold{x}}) \space (\bold{H}=\bold{I}-\bold{L})
  \end{aligned}
\end{equation}
$$
where $\bold{H}$ is high pass filter.
　In order to obtain $\hat{\bold{x}}$ of chromatgraph peaks from the observed data $\bold{y}$, We will formulate an inverse problem with the quadratic data fidelity term $\|\bold{y}-\hat{\bold{s}}\|$. Note that

$$\begin{equation}
\begin{aligned}
\|\bold{y}-\hat{\bold{s}}\|_2^2&=\|\bold{y}-\bold{Ly}-\bold{H\hat{x}}\|_2^2\\
                              &=\|\bold{H}(\bold{y}-\bold{\hat{x}})\|_2^2  
\end{aligned}
\end{equation}$$

Hence,the data fidelity term in the proposed formulation depends on the high-pass filter $\bold{H}$.Also note that the data fidelity term does not depend on the baseline estimate $\hat{\bold{f}}$. The optimization problem,formulated below. preduces an estimate of the chromatogram peaks $\hat{\bold{x}}$. The baseline estimate is then obtained by (3)

### Compound sparse derivative modeling
 According to (1),the first i derivatives, $i=0,\dotsb,M$ of the esimated peaks,should be sparse. In sparse signal processing, sparse signal behavior is typically acheved through the use of suitable non-quadratic regulation terms. Therefore, to obtain an estimate $\hat{\bold{x}}$, the following optimization problem is proposed:
 $$\begin{equation}
 \hat{\bold{x}}=arg\space \min\left(F(\bold{x})=\frac{1}{2}\|\bold{H}(\bold{y}-\bold{x})\|_2^2+\sum^M_{i=0}\lambda_i\bold{R}_i(\bold{D_i\bold{x}})\right)
 \end{equation}$$
 The assumptiion that the observed data is corruptedn by additive white Gaussian noise(AWGN) is reflected in the use of a quadratic data fitting term as in classical. The quadratic data fidelity term is given by (5).
 $\bold{D}_i$ is the order-i difference operator.
 Functions $R_i:\R^{N-i}\rightarrow\R$, are of the form
 $$\begin{equation}
   R_i(\bold{v})=\sum_n\phi(\bold{v}_n)
 \end{equation}$$
where $N_i$ denotes the length of $\bold{D_i}x$. The constants $\lambda_i\geq 0$ are regulation parameters. Increasing $\lambda_i$ has the effect of making $\bold{D}_i x$ more sparse.
 this work introduces several modifications to adapt the approach therein to chromatograms. The novel features of the approach proposed here include: 
 
 - an <em class="que">M-term compound regularizer </em> to model chromatogram peaks
 - The modeling of the positivity of chromatgram peaks by non-symmetric penalties
 - An improved algorithm based on MM for compound regularization
  
### Symmetric penalty functions

For many applications, samples of the targe signal $\bold{x}$ and its derivative $\bold{D_ix}$ are positive or negative with equal probability, or this information is not available. In such cases, the penalty function should be symmetric about $x=0$. One such function is the absolute value function,
$$\begin{equation}
  \phi_A(x)=|x|
\end{equation}$$
which leads to $l_1$ norm regularization. While widely used, one drawback of (8) is that it is non-differentialbe at zero, which can lead to numerical issues, depending on the utilized optimization algorithm. To address this issue, we utilize a smoothed approximation of the $l_1$ penalty function,for example, the hypebolic function
$$\begin{equation}
\phi_B(x)=\sqrt{|x|^2+\epsilon}
\end{equation}$$
or
$$\begin{equation}
  \phi_C(x)=|x|-\epsilon\log(|x|+\epsilon)
\end{equation}$$

In this work. They have found that $\epsilon=10^{-5}$ workds well with both (9),(10)/

### Asymmetric penalty functions
 For some applications, the signal $\bold{x}$ may be known to be sparse in an asymmetric manner. Formexample, it is known that chrotatogram data have positive peaks adove a relatively flat baseline. In such acases, it is preferable to use an asymmetric penalty function that penalizes positive and negative values diffferently.
 We start with the function $\theta:\R\rightarrow\R$ defined by

 $$\begin{equation}
 \theta(x;r)=\left\{
          \begin{array}{ll}
              x & x \geq 0 \\
              -rx & x\leq 0
          \end{array}
            \right.
 \end{equation}$$
where $r>0$ is a parameter. The function $\theta(x;r)$ is convex in $x$ and penalizes the positive and negative amplitudes asymmetrically. If r=1, then $\theta(x;r)=\phi_A(x)=|x|$
The definition (24) has the same drawback as (21), that is, it is non-differentilable at x=0. To alleviate this, a differentiable version of (24) is also propssed. In contrast to the differentiable symmetric penalties (9),(10), Thet function they propse is of the form
$$\begin{equation}
  \theta_{\epsilon}(x;r)=\left\{\begin{array}{}
  x & x>\epsilon\\
  f(x) &  |x|\leq\epsilon \\
  -rx & x< -\epsilon  
  \end{array}\right.
\end{equation}$$
with this modification, 

- $\theta_{\epsilon}(x;r)$ will remain convex in $(-\infty,+\infty)$
- $\theta_{\epsilon}(x;r)$ will be continuously differentialable on $(-\infty, +\infty)$
  
## Algorithms

### Symmetric penalty function

 The first form of the propssed approach,BEADS, is defined through the ,BEADS is defined through the minimizatiion of the objective function $F$,where $\phi$ is a differentiable symmetric penalty function. In this section,we use te MM procedure to derive an iterative algorithm for this optimization. Hence, we seek a majorizer $G(\bold{x},\bold{v})$ of $F(\bold{x})$. First, we find a majorizer $g(x,v)\text{for}\phi(x):\R\rightarrow\R \text{such that}$
 $$\begin{equation}
   g(x,v)>\phi(x)
 \end{equation}$$

 $$\begin{equation}
   g(v,v)=\phi(v), \forall x,v \in \R
 \end{equation}$$

 Since $\phi(x)$ is symmetric, we set $g(x,v)$ to be an even second-order polynomial

 $$\begin{equation}
   g(x,v)=mx^2+b
 \end{equation}$$
  
  where m and c are to be determined so as to satisfy (13),(14)
  we have
  $$g(v,v)=\phi(v)\space \text{and} \space g^{'}(v,v)=\phi^{'}(v)$$
  
  so Solving for $m$ and $b$ we obtain
  $$\begin{equation}
    m=\frac{\phi^{'}(v)}{2v}\space\text{and}\space b=\phi(v)-\frac{v}{2}\phi^{'}(v)
  \end{equation}$$
  
  Substituting (16) in (15), we obtain
  $$\begin{equation}
    g(x,v)=\frac{\phi^{'}(v)}{2v}x^2+\phi(v)-\frac{v}{2}\phi^{'}(v)
  \end{equation}$$

  which gives 
  $$\begin{equation}
  \begin{aligned}
        \sum^M_{i=0}\lambda_i\sum^{N_i-1}_{n=0}g([\bold{D_ix}]_n,[\bold{D_iv}]_n)=&\sum_{i=0}^{M}\left[\frac{\lambda_i}{2}(\bold{D_ix}^{\bold{T}})[\Lambda(\bold{D_iv})](\bold{D_ix})+c_i(\bold{v})\right]\\ &>\sum^M_{i=0}\lambda_i\sum_{n=0}^{N_i-1}\phi(\bold{D_i}x)_n
  \end{aligned}
  \end{equation}$$

  where $\Lambda(\bold{D_iv})$ are diagonal matrices,

  and $c_i(\bold{v})$ are scalars,
  $$c_i(v)=\sum_n\left[\phi([\bold{D_iv}]_n)-\frac{[\bold{D_iv}]}{2}\phi^{'}([\bold{D_iv}]_n)\right]$$
   Equality holds when $\bold{x}=\bold{v}$. Eq.(37) implies that

   $$\begin{equation}
     G(\bold{x},\bold{v})=\frac{1}{2}\|\bold{H(y-x)}\|^2_2+\sum^M_{i=0}\left[\frac{\lambda_i}{2}(\bold{D_ix})^T[\Lambda(\bold{D_i v})](\bold{D_ix})\right]+c(\bold{v})
   \end{equation}$$
   is a majorizer for F.Minimizing $G(\bold{x},\bold{v})$ with respect to $\bold{x}$ leads to the explicit solution

   <div class="note">なぜ上の式から下の式になるのかわからない</div>
   
   $$\begin{equation}
     x=\left(\bold{H}^T\bold{H}+\sum^M_{i=0}\lambda_i\bold{D_i^T}[\Lambda(\bold{D_iv})](\bold{D_ix})+\right)^{-1}\bold{H^THy}
   \end{equation}$$

   ### 補足 式(21)->式(22)
   $$\begin{equation}
     \begin{aligned}
       \frac{d}{d\bold{x}}\left(\frac{1}{2}\|\bold{H}(\boldsymbol{y}-\boldsymbol{x})\|^2\right)&=\frac{1}{2}(\bold{H}(\boldsymbol{y}-\boldsymbol{x}))^T(\bold{H}(\boldsymbol{y}-\boldsymbol{x}))\\
       &=\frac{d}{d\boldsymbol{x}}\frac{1}{2}(\boldsymbol{y}^T\bold{H}^T-\boldsymbol{x}^T\bold{H})(\bold{H}\boldsymbol{y}-\bold{H}\boldsymbol{x})\\
       &=-\bold{H}^T\bold{H}\boldsymbol{y}+\bold{H}^T\boldsymbol{H}\boldsymbol{x}
     \end{aligned}
   \end{equation}$$
   $$\begin{equation}
     \begin{aligned}
       \frac{d}{d\bold{x}}\sum^M_{i=0}\left(\frac{\lambda_i}{2}\boldsymbol{x}^T\bold{D_i}^T[\Lambda(\bold{D}_i\boldsymbol{v})](\bold{D}_i\boldsymbol{x})\right)&=\sum^M_{i=0}\left(\lambda_i\bold{D}^T_i[\Lambda(\bold{D_i}\boldsymbol{v})]\bold{D}_i\right)\boldsymbol{x}
     \end{aligned}
   \end{equation}$$
  Equation(22) explations why we use a differentiable penalty function.Suppose that the penalty funtion $\phi$ was taken to be the absolute value function,i.e$\phi(x)=|x|$,then becomes
  $$\begin{equation}
    [\lambda(\bold{D}_i)\boldsymbol{v}]_{n,n}=\frac{1}{\bold{D}_i\bold{v}}\text{sign}(\bold{D}_i\bold{v})
  \end{equation}$$
  As the iterative algorithms progress,'divid-by-zero errors' are encounterd in the implementation of (25).so use soomthed penalty function.Eq(11) or Eq(12)

  Another issue in implementing (22) resides in the computational complexity of solving the linear system represented by the matrix inverse.The computational cost increases dramatically with the length of the signal $\boldsymbol{y}$. To addre


> [ベースライン補正](https://doi.org/10.1016/j.chemolab.2014.09.014)